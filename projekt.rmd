---
title: "Izvješće"
subtitle: "Prilagodba modela podatcima i linearna regresija"
author: "Matija Bačić, Marko Lazarić, Roman Yatsukha"
date: "12.05.2019."
output: 
  pdf_document:
    includes:
      in_header: header.tex
---
  
```{r, echo = FALSE}
library(knitr)
library(pander)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

# Zadatak A

## Histogrami podataka

```{r}
dice <- read.delim("dice.dat", header=FALSE)
      
barplot(dice$V2, names.arg = dice$V1)
barplot(dice$V3, names.arg = dice$V1)

```

## Testiranje distribucije podataka, procjena parametra p

Parametar p binomne razdiobe možemo procjeniti koristeći procjenu najveće izglednosti. Kod binomne distribucije parametar p možemo procjeniti kao omjer povoljnih ishoda i svih ishoda.

```{r}

desirable.outcomes <- function(values, categories)
  sum(categories * values)

all.outcomes <- function(values, categories)
  max(categories) * sum(values)

estimate.p <- function(values, categories)
  desirable.outcomes(values, categories) / all.outcomes(values, categories)

p.0 = estimate.p(dice$V2, dice$V1)
p.1 = estimate.p(dice$V3, dice$V1)

goodness_of_fit_binomial_test <- function(frequencies, n, p) {
  print_table <- function(observed, expected, caption = 'TODO fix') {
    panderOptions('table.split.table', Inf)
    
    table.frame = data.frame(t(cbind(observed, expected)), row.names = c('$o_i$', '$e_i$'))
    colnames(table.frame) <- c(as.character(0:(length(observed) - 1)))
    pandoc.table(table.frame, style = 'rmarkdown')
  }
  
  observed = frequencies
  expected = sum(observed) * dbinom(0:n, n, p)
  
  pruned.observed = c()
  pruned.expected = c()
  current.sum.observed = 0
  current.sum.expected = 0
  current.index = 1
  
  print_table(observed, expected)
      
  for (i in 1:length(observed)) { 
    if (current.sum.expected + expected[i] > 5) {
      pruned.expected[current.index] <- current.sum.expected + expected[i]
      pruned.observed[current.index] <- current.sum.observed + observed[i]
      
      current.sum.observed = 0
      current.sum.expected = 0
      
      current.index = current.index + 1
    } 
    else {
      current.sum.observed = current.sum.observed + observed[i]
      current.sum.expected = current.sum.expected + expected[i]
    }
  }
  
  if (current.sum.expected > 0 || current.sum.observed > 0) {
    pruned.expected[current.index - 1] <- pruned.expected[current.index - 1] + current.sum.expected
    pruned.observed[current.index - 1] <- pruned.observed[current.index - 1] + current.sum.observed
  }
  
  print_table(pruned.observed, pruned.expected)
  
  X2 = sum((pruned.observed - pruned.expected)^2 / pruned.expected)
  p = pchisq(X2, df=length(pruned.observed) - 1, lower.tail = FALSE)
  
  pandoc.p(sprintf(fmt = "$$ x = \\sum\\limits^k_{i=0} \\frac{(o_i - e_i)^2}{e_i} = %.4f $$", X2))
  pandoc.p(sprintf(fmt = "$$ p = P(X \\ge x) = %.4f $$", p))
}
```

Procjena parametra p za prvi uzorak jest `r sprintf('%.4f', p.0)`, dok je za drugi uzorak `r sprintf('%.4f', p.1)`.
\pagebreak

### Testiranje prvog uzorka s distribucijom $B(12, 1/3)$

```{r results='asis'}
goodness_of_fit_binomial_test(dice$V2, 12, 1/3)
```
\pagebreak

### Testiranje prvog uzorka s distribucijom $B(12, `r sprintf('%.4f', p.0)`)$

```{r results='asis'}
goodness_of_fit_binomial_test(dice$V2, 12, p.0)
```
\pagebreak

### Testiranje drugog uzorka s distribucijom $B(12, 1/3)$

```{r results='asis'}
goodness_of_fit_binomial_test(dice$V3, 12, 1/3)
```
\pagebreak

### Testiranje drugog uzorka s distribucijom $B(12, `r sprintf('%.4f', p.1)`)$

```{r results='asis'}
goodness_of_fit_binomial_test(dice$V3, 12, p.1)
```
\pagebreak

## Usporedba histograma podataka s binomnim razdiobama

```{r}
barplot(dbinom(0:12, 12, 1 / 3), names.arg = 0:12, 
        main = "Binomna razdioba - B(12, 1 / 3)")
barplot(dbinom(0:12, 12, p.0), names.arg = 0:12, 
        main = sprintf(fmt = 'Binomna razdioba - B(12, %.4f)', p.0))
barplot(dbinom(0:12, 12, p.1), names.arg = 0:12, 
        main = sprintf(fmt = 'Binomna razdioba - B(12, %.4f)', p.1))
```

### Razlike

Binomne vjerojatnosti su pomnožene s ukupnim brojem ishoda te su od toga oduzete vrijednosti uzorka.

```{r}
barplot(sum(dice$V2) * dbinom(0:12, 12, 1 / 3) - dice$V2, names.arg = 0:12, 
        main = 'Razlika drugog uzorka i B(12, 1 / 3)')
barplot(sum(dice$V2) * dbinom(0:12, 12, p.0) - dice$V2, names.arg = 0:12, 
        main = sprintf(fmt = 'Razlika prvog uzorka i B(12, %.4f)', p.0))

barplot(sum(dice$V3) * dbinom(0:12, 12, 1 / 3) - dice$V3, names.arg = 0:12, 
        main = 'Razlika drugog uzorka i B(12, 1 / 3)')
barplot(sum(dice$V3) * dbinom(0:12, 12, p.1) - dice$V3, names.arg = 0:12, 
        main = sprintf(fmt = 'Razlika drugog uzorka i B(12, %.4f)', p.1))
```

## Određivanje intervala pouzdanosti za parametar p

Koristeći Clopper-Pearson intervalnu procjenu možemo procjeniti interval pouzdanosti parametra p. 

```{r}

confidence.interval <- function(values, categories, interval) {
  alpha.half <- (1 - interval) / 2
  
  x <- desirable.outcomes(values, categories)
  n <- all.outcomes(values, categories)
  
  c(qbeta(alpha.half, x, n - x + 1), qbeta(1 - alpha.half, x + 1, n - x))
}

interval = 0.95

cp.0 = confidence.interval(dice$V2, dice$V1, interval)
cp.1 = confidence.interval(dice$V3, dice$V1, interval)

```

`r interval * 100`% interval pouzdanosti za prvi uzorak jest: [`r sprintf('%.4f, %.4f', cp.0[1], cp.0[2])`], dok je za drugi uzorak: [`r sprintf('%.4f, %.4f', cp.1[1], cp.1[2])`].

# Zadatak B

```{r}
library(ggplot2)
forbes <- read.table("forbes.dat", quote="\"", comment.char="")
hooker <- read.table("hooker.dat", quote="\"", comment.char="")
fh_concat <- rbind(forbes, hooker)

names(fh_concat)[names(fh_concat) == "V1"] <- "Vreliste"
names(fh_concat)[names(fh_concat) == "V2"] <- "Tlak"

plot(fh_concat$Vreliste, fh_concat$Tlak)

# Prikazom podataka, vidimo da graf sugerira linearnu vezu među podacima.

linear_model <- lm(Vreliste ~ Tlak, data = fh_concat)
linear_model

# Linearni model:

ggplot(fh_concat, aes(Tlak, Vreliste)) +
  geom_point() +
  stat_smooth(method = lm, size = 1, se = FALSE)

Tlak2 <- fh_concat$Tlak^2
quadratic_model <-lm(Vreliste ~ Tlak + Tlak2, data = fh_concat)
quadratic_model

# Kvadratični model:

ggplot(fh_concat, aes(x = Tlak, y = Vreliste)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1, se = FALSE)

# Reziduali pojedinih modela:
fh_concat$predictedLinear = predict(linear_model)
fh_concat$residualsLinear = residuals(linear_model)
fh_concat$stdResidualsLinear = rstandard(linear_model)
fh_concat$predictedQuadratic = predict(quadratic_model)
fh_concat$residualsQuadratic = residuals(quadratic_model)
fh_concat$stdResidualsQuadratic = rstandard(quadratic_model)
x.axis = seq(from = 0, to = nrow(fh_concat) - 1, by = 1)

# Prikaz reziduala linearnog modela
ggplot(data=fh_concat, mapping=aes(x = x.axis, y = residualsLinear)) + 
  geom_point()

# Prikaz standardnih reziduala linearnog modela
ggplot(data=fh_concat, mapping=aes(x = x.axis, y = stdResidualsLinear)) + 
  geom_point()

# Prikaz reziduala kvadratičnog modela
ggplot(data=fh_concat, mapping=aes(x = x.axis, y = residualsQuadratic)) + 
  geom_point()

# Prikaz standardnih reziduala kvadratičnog modela
ggplot(data=fh_concat, mapping=aes(x = x.axis, y = stdResidualsQuadratic)) + 
  geom_point()

# Provjera normalnosti reziduala linearnog modela (normalni vjerojatnosni graf)
qqnorm(fh_concat$stdResidualsLinear, xlab="Standardizirani reziduali", ylab="Z-vrijednost", main="Reziduali linearne regresije (normalni vjerojatnosni graf)"); qqline(fh_concat$stdResidualsLinear)

# Provjera normalnosti reziduala kvadratičnog modela (normalni vjerojatnosni graf)
qqnorm(fh_concat$stdResidualsQuadratic, xlab="Standardizirani reziduali", ylab="Z-vrijednost", main="Reziduali kvadratne regresije (normalni vjerojatnosni graf)"); qqline(fh_concat$stdResidualsQuadratic)

# Provjera normalnosti reziduala linearnog modela (Kolmogorov-Smirnovljev test)
ks.test(fh_concat$stdResidualsLinear, "pnorm", mean=mean(fh_concat$stdResidualsLinear), sd=sd(fh_concat$stdResidualsLinear))

# Provjera normalnosti reziduala kvadratičnog modela (Kolmogorov-Smirnovljev test)
ks.test(fh_concat$stdResidualsQuadratic, "pnorm", mean=mean(fh_concat$stdResidualsQuadratic), sd=sd(fh_concat$stdResidualsQuadratic))

# Gledajući i po normalnom vjerojatnosnom grafu i po Kolmogorov-Smirnovljevom testu, reziduali kvadratičnog modela ipak malo bolje odgovaraju jediničnoj normalnoj distribuciji. Iako, i jedni i drugi reziduali imaju dosta visoku p vrijednost.

# R^2 statisticka
summary(linear_model)$r.squared
summary(quadratic_model)$r.squared

# U ovom koraku, zaključili bismo da nema toliko značajne razlike da nadmaši kompleksnost kvadratičnog modela (sudeći po R^2 statistici). Linearni model ipak bi bio pogodniji. Ipak, ne možemo još to sa sigurnošću zaključiti bez dodatnih informacija ANOVA-e.

# Anova test prihvatljivosti modela

anova(linear_model, quadratic_model)

# U ovom koraku, dobivamo više informacija. Uz nivo značajnosti 0.05 i uzimajući u obzir p < 2.2e-16, možemo zaključiti da dobivamo statistički značajan napredak koristeći kvadratični u odnosnu na linearni model.

# 95% interval povjerenja za paramtere kvadratičnog modela:

summary(quadratic_model)
confint(quadratic_model, 'Tlak', level=0.95)
confint(quadratic_model, 'Tlak2', level=0.95)
confint(quadratic_model, '(Intercept)', level=0.95)

confident <- data.frame(Tlak = fh_concat$Tlak, Vreliste = predict(quadratic_model, fh_concat, interval = 'confidence', level = 0.95))

ggplot(fh_concat, aes(x = Tlak, y = Vreliste)) +
  geom_point() +
  geom_smooth(data = confident, aes(x = Tlak, y = Vreliste.lwr), color = 'red') + 
  geom_smooth(data = confident, aes(x = Tlak, y = Vreliste.upr), color = 'blue')

predicted <- data.frame(Tlak = fh_concat$Tlak, Vreliste = predict(quadratic_model, fh_concat, interval = 'prediction', level = 0.95))

ggplot(fh_concat, aes(x = Tlak, y = Vreliste)) +
  geom_point() +
  geom_smooth(data = predicted, aes(x = Tlak, y = Vreliste.lwr), color = 'red') + 
  geom_smooth(data = predicted, aes(x = Tlak, y = Vreliste.upr), color = 'blue')

```